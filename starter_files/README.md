

# Operationalizing Machine Learning with Azure

In this project we first configure an Auto ML experiment in Azure ML Studio for a bank marketing dataset with the aim of predicting if a certain individual will make a term deposit. After the experiment has been completed, we deploy the best performing model to an Azure Container Instance and then consume the endpoint with a JSON payload. Afterwards, we create a automated pipeline using Azure SDK to develop the same AutoML model, publish it, and then consume it.

## Architectural Diagram

### The project consists of the following steps:

### Authentication: 
We need to authenticate our credentials and what we are allowed to do with Azure ML. This is automatically taken care of through the Udacity subscription.

### Automated ML Experiment:
We run a classification AutoML experiment on the Bank Marketing dataset and then retrieve the best performing model

### Deploy the best model
We deploy the best performing model on an Azure Container Instance with authentication enabled.

### Enable logging
We enable Application Insights for detailed logging and debugging purposes.

### Swagger Documentation
We create a Swagger UI to document our endpoint API in order to make it easier for users to understand the various http requests.

### Consume model endpoints
We consume the model endpoint with a JSON payload

### Create and publish a pipeline
We automate the aforementioned steps by creating and publishing an AutoML pipeline.

### Documentation
Everything is documented in a readme file.

### Architecture Diagram of all the Steps
![image](https://user-images.githubusercontent.com/38438203/121116307-4f30c800-c7e4-11eb-92e3-8078b006f251.png)

## Key Steps

### Automated ML Experiment

  1. We use the registered bank marketing dataset
  ![image](https://user-images.githubusercontent.com/38438203/121117947-fc0c4480-c7e6-11eb-8f75-12b5edde0203.png)
  
  2. Run the classification AutoML Experiment
  ![image](https://user-images.githubusercontent.com/38438203/121118109-4988b180-c7e7-11eb-89ab-742c740db86d.png)

  3. Retrieve the best performing model which is a Voting Ensemble model
  ![image](https://user-images.githubusercontent.com/38438203/121118210-76d55f80-c7e7-11eb-89a6-e992cb8b07b8.png)
  
  ![image](https://user-images.githubusercontent.com/38438203/121118245-83f24e80-c7e7-11eb-9076-d0035c3a408f.png)
  
### Deploy the best performing model

  1. We deploy the best performing model on an Azure Container Instance with authentication enabled
  ![image](https://user-images.githubusercontent.com/38438203/121118620-2b6f8100-c7e8-11eb-8d9b-ff3012c8806f.png)

### Enable Logging

  1. We enable logging for the deployed model in Azure SDK using the logs.py script
  ![image](https://user-images.githubusercontent.com/38438203/121118826-85704680-c7e8-11eb-81ef-0b716ca40902.png)
  
  2. We can see the logs generated by the deployed model through our script 
  ![image](https://user-images.githubusercontent.com/38438203/121119074-fdd70780-c7e8-11eb-9095-00f706969078.png)

### Swagger Documentation

  1. We create a Swagger UI using swagger.sh, swagger.json, and serve.py to document our model endpoint's HTTP requests in a user friendly HTML format
  ![image](https://user-images.githubusercontent.com/38438203/121119313-62926200-c7e9-11eb-9e6e-0259a9a87cbb.png)
  
  2. The endpoint has a GET request which can be used for debugging
  ![image](https://user-images.githubusercontent.com/38438203/121119364-7a69e600-c7e9-11eb-8777-57beb636b378.png)
  
  3. The endpoint has a POST Request to obtain predictions for user given inputs
  ![image](https://user-images.githubusercontent.com/38438203/121119463-ac7b4800-c7e9-11eb-9bc5-3a332ce325b7.png)

### Consume Model Endpoints

  1. We consume our model's endpoint by using endpoint.py script which passes two instances to the model for predictions
  ![image](https://user-images.githubusercontent.com/38438203/121119713-21e71880-c7ea-11eb-87aa-131bd807b6e1.png)

### Create, Publish, and Consume Pipeline

  1. We create a pipeline using Azure SDK in a Jupyter Notebook to automate all the aforementioned steps
  ![image](https://user-images.githubusercontent.com/38438203/121120197-16482180-c7eb-11eb-8d76-be05032bc041.png)

  2. We also create a pipeline endpoint for automated consumption
  ![image](https://user-images.githubusercontent.com/38438203/121120318-53141880-c7eb-11eb-8b31-82286fa2a51d.png)

  3. We use the same bank marketing dataset
  ![image](https://user-images.githubusercontent.com/38438203/121120427-82c32080-c7eb-11eb-8328-d616317520a4.png)
  
  4. This image shows the pipeline REST endpoint as being active
  ![image](https://user-images.githubusercontent.com/38438203/121120552-c6b62580-c7eb-11eb-9763-cf60266499f4.png)

  5. These are images of the "Use RunDetails Widget"
  ![image](https://user-images.githubusercontent.com/38438203/121120655-00872c00-c7ec-11eb-8aa5-1a7fc4d23b5e.png)
  ![image](https://user-images.githubusercontent.com/38438203/121120672-0977fd80-c7ec-11eb-8e7f-7e2b6e5fed8d.png)
  ![image](https://user-images.githubusercontent.com/38438203/121120759-39bf9c00-c7ec-11eb-88ff-3435808c837a.png)
  ![image](https://user-images.githubusercontent.com/38438203/121120779-42b06d80-c7ec-11eb-9ac4-8ecc2fc8e6c8.png)

  6. Image of the pipeline scheduled runs in ML Studio
  ![image](https://user-images.githubusercontent.com/38438203/121120988-bc485b80-c7ec-11eb-8ac1-d5367e6b01d9.png)
  
  7. Image of the best model run from the RunDetailsWidget
  ![image](https://user-images.githubusercontent.com/38438203/121121208-25c86a00-c7ed-11eb-8ad7-3acafb546990.png)


## Screen Recording

A screencast of the project maybe viewed at https://youtu.be/qUbXFuok81c


